{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import permutation_test_score as p_test\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load vectors directly from the file\n",
    "model = KeyedVectors.load_word2vec_format('/Users/yiwen/Documents/coding_practise/blc/mini-dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# Access vectors for specific words with a keyed lookup:\n",
    "vector = model['easy']\n",
    "# see the shape of the vector (300,)\n",
    "vector.shape\n",
    "# Processing sentences is not as simple as with Spacy:\n",
    "vectors = [model[x] for x in \"This is some text I am processing with Spacy\".split(' ')]\n",
    "\n",
    "def open_list(path):\n",
    "    lst = []\n",
    "    with open(path,'r') as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            lst.append(line)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 65 58 31\n",
      "Test set:  40 28 19\n"
     ]
    }
   ],
   "source": [
    "# three categories\n",
    "sup_train, basic_train, sub_train = [],[],[]\n",
    "sup_train_label,basic_train_label,sub_train_label = [],[],[]\n",
    "\n",
    "\n",
    "sup_test, basic_test, sub_test = [],[],[]\n",
    "sup_test_label,basic_test_label,sub_test_label = [],[],[]\n",
    "\n",
    "X_train, X_test, y_train, y_test = [],[],[],[]\n",
    "\n",
    "path = '/Users/yiwen/Documents/coding_practise/blc/bond/labels.txt'\n",
    "with open(path,'r') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        line = line.split('\\t')\n",
    "        label,subset,word = line[0],line[1],line[2]\n",
    "        if label == 'subordinate':\n",
    "            if subset != 'test':\n",
    "                sub_train.append(word)\n",
    "                sub_train_label.append(3)\n",
    "#                 sub_train_label.append(0)\n",
    "            else:\n",
    "                sub_test.append(word)\n",
    "                sub_test_label.append(3)\n",
    "#                 sub_test_label.append(0)\n",
    "        elif label == 'basic':\n",
    "            if subset != 'test':\n",
    "                basic_train.append(word)\n",
    "                basic_train_label.append(2)\n",
    "            else:\n",
    "                basic_test.append(word)\n",
    "                basic_test_label.append(2)\n",
    "        else:\n",
    "            if subset != 'test':\n",
    "                sup_train.append(word)\n",
    "                sup_train_label.append(1)\n",
    "#                 sup_train_label.append(0)\n",
    "            else:\n",
    "                sup_test.append(word)\n",
    "                sup_test_label.append(1)\n",
    "#                 sup_test_label.append(0)\n",
    "                \n",
    "print('Training set:',len(sub_train),len(basic_train),len(sup_train))\n",
    "print('Test set: ', len(sub_test),len(basic_test),len(sup_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Four categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 65 58 31 94\n",
      "Test set:  40 28 19 98\n"
     ]
    }
   ],
   "source": [
    "# four categories\n",
    "sup_train, basic_train, sub_train, abst_train = [],[],[],[]\n",
    "sup_train_label,basic_train_label,sub_train_label,abst_train_label = [],[],[],[]\n",
    "\n",
    "\n",
    "sup_test, basic_test, sub_test, abst_test = [],[],[],[]\n",
    "sup_test_label,basic_test_label,sub_test_label,abst_test_label = [],[],[],[]\n",
    "\n",
    "X_train, X_test, y_train, y_test = [],[],[],[]\n",
    "\n",
    "# path = '/Users/yiwen/Documents/coding_practise/blc/bond/labels2.txt'\n",
    "path = '/Users/yiwen/Documents/coding_practise/blc/medium-dataset-train-test-set.txt'\n",
    "with open(path,'r') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        line = line.split('\\t')\n",
    "        label,subset,word = line[0],line[1],line[2]\n",
    "        if label == 'subordinate':\n",
    "            if subset != 'test':\n",
    "                sub_train.append(word)\n",
    "#                 sub_train_label.append(3)\n",
    "                sub_train_label.append(3)\n",
    "            else:\n",
    "                sub_test.append(word)\n",
    "#                 sub_test_label.append(3)\n",
    "                sub_test_label.append(3)\n",
    "        elif label == 'basic':\n",
    "            if subset != 'test':\n",
    "                basic_train.append(word)\n",
    "                basic_train_label.append(2)\n",
    "            else:\n",
    "                basic_test.append(word)\n",
    "                basic_test_label.append(2)\n",
    "        elif label == 'superordinate':\n",
    "            if subset != 'test':\n",
    "                sup_train.append(word)\n",
    "                sup_train_label.append(1)\n",
    "            else:\n",
    "                sup_test.append(word)\n",
    "                sup_test_label.append(1)\n",
    "        else:\n",
    "            if subset != 'test':\n",
    "                abst_train.append(word)\n",
    "#                 sub_train_label.append(1)\n",
    "                abst_train_label.append(0)\n",
    "            else:\n",
    "                abst_test.append(word)\n",
    "#                 sup_test_label.append(1)\n",
    "                abst_test_label.append(0)\n",
    "                \n",
    "print('Training set:',len(sub_train),len(basic_train),len(sup_train),len(abst_train))\n",
    "print('Test set: ', len(sub_test),len(basic_test),len(sup_test),len(abst_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_basic: 32\n",
      "Train sub 20\n",
      "Train sup 14\n",
      "Train abst 94\n",
      "Test basic 28\n",
      "Test sub 40\n",
      "Test sup 19\n",
      "Test abst 98\n",
      "Dev basic 26\n",
      "Dev sub 45\n",
      "Dev sup 17\n",
      "Dev abst 0\n"
     ]
    }
   ],
   "source": [
    "train_ba,train_sub,train_sup,train_abst = [],[],[],[]\n",
    "test_ba,test_sub,test_sup,test_abst = [],[],[],[]\n",
    "dev_ba,dev_sub,dev_sup,dev_abst = [],[],[],[]\n",
    "with open('/Users/yiwen/Documents/coding_practise/blc/medium-dataset-train-test-set.txt','r') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        line = line.split('\\t')\n",
    "        label,subset,word = line[0],line[1],line[2]\n",
    "        if label == 'superordinate':\n",
    "            if subset == 'train':\n",
    "                train_sup.append(word)\n",
    "            elif subset == 'dev':\n",
    "                dev_sup.append(word)\n",
    "            else:\n",
    "                test_sup.append(word)\n",
    "        elif label == 'subordinate':\n",
    "            if subset == 'train':\n",
    "                train_sub.append(word)\n",
    "            elif subset == 'dev':\n",
    "                dev_sub.append(word)\n",
    "            else:\n",
    "                test_sub.append(word)\n",
    "        elif label == 'basic':\n",
    "            if subset == 'train':\n",
    "                train_ba.append(word)\n",
    "            elif subset == 'dev':\n",
    "                dev_ba.append(word)\n",
    "            else:\n",
    "                test_ba.append(word)   \n",
    "        else:\n",
    "            if subset == 'train':\n",
    "                train_abst.append(word)\n",
    "            elif subset == 'dev':\n",
    "                dev_abst.append(word)\n",
    "            else:\n",
    "                test_abst.append(word) \n",
    "                \n",
    "                \n",
    "print('Training_basic:',len(train_ba))\n",
    "print('Train sub',len(train_sub))\n",
    "print('Train sup',len(train_sup))\n",
    "print('Train abst',len(train_abst))\n",
    "print('Test basic',len(test_ba))\n",
    "print('Test sub',len(test_sub))\n",
    "print('Test sup',len(test_sup))\n",
    "print('Test abst',len(test_abst))\n",
    "print('Dev basic',len(dev_ba))\n",
    "print('Dev sub',len(dev_sub))\n",
    "print('Dev sup',len(dev_sup))\n",
    "print('Dev abst',len(dev_abst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n",
      "185\n"
     ]
    }
   ],
   "source": [
    "# 4-way\n",
    "mini_synsets = sub_train + basic_train + sup_train + abst_train\n",
    "print(len(mini_synsets))\n",
    "test_synsets = sub_test + basic_test + sup_test + abst_test\n",
    "print(len(test_synsets))\n",
    "\n",
    "#3-way\n",
    "# mini_synsets = sub_train + basic_train + sup_train\n",
    "# print(len(mini_synsets))\n",
    "# test_synsets = sub_test + basic_test + sup_test\n",
    "# print(len(test_synsets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "58\n",
      "31\n",
      "94\n",
      "248\n",
      "40\n",
      "28\n",
      "19\n",
      "98\n",
      "185\n"
     ]
    }
   ],
   "source": [
    "c = 0 \n",
    "for x in [sub_train, basic_train, sup_train, abst_train]:\n",
    "    c += len(x)\n",
    "    print(len(x))\n",
    "    \n",
    "print(c)\n",
    "\n",
    "c = 0 \n",
    "for x in [sub_test, basic_test, sup_test, abst_test]:\n",
    "    c += len(x)\n",
    "    print(len(x))\n",
    "    \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.002309'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of feature\n",
    "word_num = dict()\n",
    "# with open('/Users/yiwen/Documents/coding_practise/blc/Final_dataset/Buch_40_noun_tagged.csv','r') as fin:\n",
    "# with open('/Users/yiwen/Documents/coding_practise/blc/mbart/synset_WSD_first_sense_0129_82115.csv','r') as fin:\n",
    "\n",
    "# DM + BART features\n",
    "# with open('/Users/yiwen/Documents/coding_practise/blc/full_type_DM/DM_BART_82115.csv','r') as fin:\n",
    "with open('synset_WSD_first_sense_0129_82115.csv','r') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        line = line.split(';')\n",
    "        word = line[0]\n",
    "        features = line[1:]\n",
    "        word_num[word] = len(features)\n",
    "\n",
    "#frequency\n",
    "fre_dict = dict()\n",
    "\n",
    "with open('/Users/yiwen/Documents/coding_practise/blc/1gramsbyfreq_google_book.txt','r') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        line = line.split('\\t')\n",
    "        word, num = line[0],line[1]\n",
    "        fre_dict[word] = num\n",
    "        \n",
    "\n",
    "#pagerank score\n",
    "pg_score = dict()\n",
    "\n",
    "# with open('/Users/yiwen/Documents/coding_practise/blc/mbart/pr_result_82115.csv','r') as fin:\n",
    "# \n",
    "# DM+BART PageRank score\n",
    "# with open('/Users/yiwen/Documents/coding_practise/blc/full_type_DM/pr_dm_bart_82115.csv','r') as fin:\n",
    "with open('/Users/yiwen/Documents/coding_practise/blc/mbart/pr_1st_WSD_82115.csv','r') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        line = line.split(',')\n",
    "        word, pg = line[0], line[3]\n",
    "        pg_score[word] = pg\n",
    "        \n",
    "pg_score['fish.n.01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n"
     ]
    }
   ],
   "source": [
    "# depth in WN\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "syn_depth = dict()\n",
    "\n",
    "for m in mini_synsets:\n",
    "    s = wn.synset(m)\n",
    "    dp = s.max_depth()\n",
    "    syn_depth[m] = float(dp)\n",
    "    \n",
    "    \n",
    "print(len(syn_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bottom\n",
    "syn_hypo = dict()\n",
    "def GetMaxHeight(synset):\n",
    "    '''\n",
    "    returns max height of the tree (from bottom, choosing longest hyponym path)\n",
    "    '''\n",
    "    hs = wn.synset(synset).hyponyms()\n",
    "    if len(hs) < 1:\n",
    "        return 1.0\n",
    "    max_height = 1.0\n",
    "    for h in hs:\n",
    "        d = GetMaxHeight(h.name()) + 1.0\n",
    "        if d > max_height:\n",
    "            max_height = d\n",
    "    return max_height\n",
    "\n",
    "# all_synsets = wn.all_synsets('n')\n",
    "# for a in all_synsets:\n",
    "#     syn_hypo[m] = GetMaxHeight(a)\n",
    "    \n",
    "for m in mini_synsets:\n",
    "    syn_hypo[m] = GetMaxHeight(m)\n",
    "\n",
    "\n",
    "#abstractness rating\n",
    "Ac_rating = dict()\n",
    "\n",
    "with open('/Users/yiwen/Documents/coding_practise/blc/Abstract_EN_ratings_AoA/AC_ratings_google3m_koeper_SiW.csv','r') as fin:\n",
    "    heading = True\n",
    "    for line in fin:\n",
    "        if heading == True:\n",
    "            heading = False\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        line = line.split('\\t')\n",
    "        try:\n",
    "            Ac_rating[line[0]] = line[1]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "\n",
    "def bart_feature():\n",
    "    bart_fea = dict()\n",
    "#     with open('synset_WSD_0120_82115.csv','r') as fin:\n",
    "    with open('synset_WSD_first_sense_0129_82115.csv','r') as fin:\n",
    "#DM BART \n",
    "#     with open('/Users/yiwen/Documents/coding_practise/blc/full_type_DM/DM_BART_82115.csv','r') as fin:\n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                line = line.split(';')\n",
    "                word,features = line[0],line[1:]\n",
    "                if word in features:\n",
    "                    features.remove(word)\n",
    "                bart_fea[word] = len(set(features))\n",
    "    return bart_fea\n",
    "\n",
    "bart_result = bart_feature()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_child():\n",
    "    early = dict()\n",
    "    with open('/Users/yiwen/Documents/coding_practise/blc/mini-dataset/infant_toddler.txt','r') as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            line = line.split('\\t')\n",
    "            word,sco = line[0],line[1]\n",
    "            early[word] = float(sco)/100    \n",
    "    return early\n",
    "\n",
    "early_learnt = check_child()\n",
    "\n",
    "def cue_validity():\n",
    "    cue_s = dict()\n",
    "#     with open('cue_validity_82115.csv','r') as fin:\n",
    "    with open('cue_validity_82115_1st.csv','r') as fin:\n",
    "#     with open('cue_validity_dm_bart_82115.csv','r') as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            line = line.split('\\t')\n",
    "            word,sco = line[0],line[1]           \n",
    "            cue_s[word] = sco\n",
    "    return cue_s\n",
    "\n",
    "bartcue_word = cue_validity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoa = dict()\n",
    "with open('/Users/yiwen/Documents/coding_practise/blc/Abstract_EN_ratings_AoA/synset_first_sense_aoa.csv','r') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        line = line.split('\\t')\n",
    "        aoa[line[0]] = line[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add AoA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracted(word,size,bart,frequency,pagerank,depth,hypo,abrating,child,cue,age,w2v_model):\n",
    "    threshold = 0.001\n",
    "    w = word.split('.')[0]\n",
    "    lst = np.zeros(size)\n",
    "    for i in range(lst.shape[0]):\n",
    "        try:#number of features\n",
    "            lst[0] = float(bart[word])\n",
    "        except KeyError:\n",
    "            lst[0] = threshold\n",
    "        try:# frequency in Google Books\n",
    "            lst[1] = float(frequency[w])\n",
    "        except KeyError:\n",
    "            lst[1] = threshold\n",
    "        try:\n",
    "            lst[2] = float(pagerank[word])\n",
    "        except KeyError:\n",
    "            lst[2] = threshold\n",
    "        try:\n",
    "            lst[3] = depth[word]\n",
    "        except KeyError:\n",
    "            lst[3] = threshold\n",
    "        try:\n",
    "            lst[4] = hypo[word]\n",
    "        except KeyError:\n",
    "            lst[4] = threshold\n",
    "        try:\n",
    "            lst[5] = float(abrating[w])\n",
    "        except KeyError:\n",
    "#             lst[5] = threshold\n",
    "            lst[5] = 15\n",
    "        try:\n",
    "            lst[6] = child[w]\n",
    "        except KeyError:\n",
    "            lst[6] = threshold\n",
    "        try:\n",
    "            lst[7] = cue[word]\n",
    "        except KeyError:\n",
    "            lst[7] = threshold\n",
    "        # less characters\n",
    "        try:\n",
    "            lst[8] = len(word)\n",
    "        except KeyError:\n",
    "            lst[8] = len(word)\n",
    "        try:\n",
    "            lst[9] = age[word]\n",
    "        except KeyError:\n",
    "            lst[9] = threshold\n",
    "#         try:\n",
    "#             lst[10] = edu[word]\n",
    "#         except KeyError:\n",
    "#             lst[10] = 10\n",
    "        try:\n",
    "            lst[10:] = w2v_model[w]\n",
    "        except KeyError:\n",
    "            lst[10:] = np.zeros(lst[10:].shape)\n",
    "#             lst[10:] = np.ones(lst[10:].shape)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has processed  0  words\n",
      "Has processed  50  words\n",
      "Has processed  100  words\n",
      "Has processed  150  words\n",
      "Has processed  200  words\n",
      "(248, 310)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.zeros((len(mini_synsets),310))\n",
    "# print(dataset)\n",
    "for i, m in enumerate(mini_synsets):\n",
    "    features = extracted(m,dataset.shape[1],bart_result,fre_dict,pg_score,syn_depth,syn_hypo,Ac_rating,early_learnt,bartcue_word,aoa,model)\n",
    "    dataset[i,:] = features\n",
    "    if i%50 == 0:\n",
    "        print('Has processed ',i,' words')\n",
    "        \n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_synsets = list(wn.all_synsets('n'))\n",
    "all_synsets2 = []\n",
    "for a in all_synsets:\n",
    "    all_synsets2.append(a.name())\n",
    "\n",
    "all_synsets2[0]\n",
    "\n",
    "syn_hypo = dict()\n",
    "for m in all_synsets2:\n",
    "    syn_hypo[m] = GetMaxHeight(m)\n",
    "\n",
    "syn_depth = dict()\n",
    "\n",
    "for m in all_synsets2:\n",
    "    s = wn.synset(m)\n",
    "    dp = s.max_depth()\n",
    "    \n",
    "    syn_depth[m] = dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "all_synsets = list(wn.all_synsets('n'))\n",
    "all_synsets[0].name()\n",
    "\n",
    "with open('synsets_noun20220405.txt','w') as fout:\n",
    "    for a in all_synsets:\n",
    "        fout.write(a.name() + '\\n')\n",
    "\n",
    "#dataset_all = np.zeros((len(all_synsets2),309))\n",
    "dataset_all = np.zeros((len(all_synsets2),310))\n",
    "# print(dataset)\n",
    "for i, m in enumerate(all_synsets2):\n",
    "    features = extracted(m,dataset_all.shape[1],bart_result,fre_dict,pg_score,syn_depth,syn_hypo,Ac_rating,early_learnt,bartcue_word,aoa,model)\n",
    "    dataset_all[i,:] = features\n",
    "    if i%10000 == 0:\n",
    "        print('Has processed ',i,' words')\n",
    "        \n",
    "print(dataset_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test (brain-dead version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_best_model_output = 999\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.evaluate import permutation_test\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "w2v = True\n",
    "log = True\n",
    "processing_missing_data = True\n",
    "\n",
    "if w2v:\n",
    "    dataset2 = dataset\n",
    "else:\n",
    "    dataset2 = dataset[:,:10]#number changes\n",
    "\n",
    "# process missing data, replace missing data with median of all data\n",
    "# no accuracy improvement\n",
    "if processing_missing_data:\n",
    "    for i in range(dataset2.shape[1]):\n",
    "        median = np.median(dataset2[:,i][dataset2[:,i]!=0])\n",
    "        dataset2[:,i][dataset2[:,i]==0] = median\n",
    "    \n",
    "dataset3 = np.zeros(dataset2.shape)\n",
    "for i in range(dataset2.shape[1]):\n",
    "    # take log of frequency and pagerank data\n",
    "    if log:\n",
    "        if i in [1,2,7,8]:\n",
    "            dataset3[:,i] = -np.log(dataset2[:,i])\n",
    "            dataset3[:,i] = (dataset3[:,i]-np.min(dataset3[:,i]))/(np.max(dataset3[:,i])-np.min(dataset3[:,i]))\n",
    "            continue\n",
    "    dataset3[:,i] = (dataset2[:,i]-np.min(dataset2[:,i]))/(np.max(dataset2[:,i])-np.min(dataset2[:,i]))\n",
    "\n",
    "X = dataset3\n",
    "y = np.array([3]*len(sub_train)+[2]*len(basic_train)+[1]*len(sup_train) + [0]*len(abst_train))\n",
    "# print(X.shape,y.shape)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# clf = OneVsRestClassifier(SVC()).fit(X_train, y_train)\n",
    "# clf = OneVsRestClassifier(SVC(class_weight='balanced')).fit(X, y)\n",
    "\n",
    "# scores = cross_val_score(clf, X, y, cv=10)*100\n",
    "# print('accuracy: {:0.2f} ± {:0.2f} %'.format(np.mean(scores), scipy.stats.sem(scores)))\n",
    "\n",
    "\n",
    "\n",
    "# Change number according to the features\n",
    "#\n",
    "# \n",
    "dataset_test = np.zeros((len(test_synsets),310))#number changes\n",
    "# print(dataset)\n",
    "for i, m in enumerate(test_synsets):\n",
    "    features = extracted(m,dataset.shape[1],bart_result,fre_dict,pg_score,syn_depth,syn_hypo,Ac_rating,early_learnt,bartcue_word,aoa,model)\n",
    "    dataset_test[i,:] = features\n",
    "#     if i%50 == 0:\n",
    "#         print('Has processed ',i,' words')\n",
    "# print(dataset_test.shape)\n",
    "\n",
    "if w2v:\n",
    "    dataset2 = dataset_test\n",
    "else:\n",
    "    dataset2 = dataset_test[:,:10]\n",
    "\n",
    "# process missing data, replace missing data with median of all data\n",
    "# no accuracy improvement\n",
    "if processing_missing_data:\n",
    "    for i in range(dataset2.shape[1]):\n",
    "        median = np.median(dataset2[:,i][dataset2[:,i]!=0])\n",
    "        dataset2[:,i][dataset2[:,i]==0] = median\n",
    "    \n",
    "\n",
    "dataset3 = np.zeros(dataset2.shape)\n",
    "for i in range(dataset2.shape[1]):\n",
    "    # take log of frequency and pagerank data\n",
    "    if log:\n",
    "        if i in [1,2,3,4,6,7,8]:\n",
    "            dataset3[:,i] = -np.log(dataset2[:,i])\n",
    "            dataset3[:,i] = (dataset3[:,i]-np.min(dataset3[:,i]))/(np.max(dataset3[:,i])-np.min(dataset3[:,i]+1e-10))\n",
    "            continue\n",
    "    dataset3[:,i] = (dataset2[:,i]-np.min(dataset2[:,i]))/(np.max(dataset2[:,i])-np.min(dataset2[:,i]+1e-10))\n",
    "    \n",
    "    \n",
    "    \n",
    "y_test = np.array([3]*len(sub_test) + [2]*len(basic_test) + [1]*len(sup_test) + [0]*len(abst_test))#number changes\n",
    "\n",
    "\n",
    "X_test = dataset3\n",
    "\n",
    "### check which feature is the most important\n",
    "### by setting other data to 0\n",
    "#0:num of features,1: frequency in corpus,2:pagerank\n",
    "#3:depth in WN,4: from bottom,5:abstractness rating,6:early learn 7:cue validity 8: number of characters 9:word2vec\n",
    "\n",
    "for data in [X, X_test]:\n",
    "#      data[:,0] = 0\n",
    "#      data[:,1] = 0\n",
    "#      data[:,2] = 0\n",
    "#      data[:,3] = 0\n",
    "#      data[:,4] = 0\n",
    "     data[:,5] = 0\n",
    "     data[:,6] = 0\n",
    "#      data[:,7] = 0\n",
    "#      data[:,8] = 0\n",
    "#      data[:,9:] = 0\n",
    "#      data[:,10] = 0\n",
    "#      data[:,11:] = 0\n",
    "     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8972    0.9796    0.9366        98\n",
      "           1     0.8333    0.2632    0.4000        19\n",
      "           2     0.5833    0.7500    0.6562        28\n",
      "           3     0.8889    0.8000    0.8421        40\n",
      "\n",
      "    accuracy                         0.8324       185\n",
      "   macro avg     0.8007    0.6982    0.7087       185\n",
      "weighted avg     0.8413    0.8324    0.8186       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# revised codes\n",
    "clf = OneVsRestClassifier(SVC()).fit(X, y)\n",
    "y_pred = clf.predict(X_test)\n",
    "result = classification_report(y_test, y_pred, digits=4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(max_iter=15000, tol=0.01)\n",
      "[[65  7 26  0]\n",
      " [ 0  8 11  0]\n",
      " [ 0  9 17  2]\n",
      " [ 0  3 30  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.6633    0.7975        98\n",
      "           1     0.2963    0.4211    0.3478        19\n",
      "           2     0.2024    0.6071    0.3036        28\n",
      "           3     0.7778    0.1750    0.2857        40\n",
      "\n",
      "    accuracy                         0.5243       185\n",
      "   macro avg     0.5691    0.4666    0.4337       185\n",
      "weighted avg     0.7590    0.5243    0.5659       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgdc = SGDClassifier(max_iter=15000, tol=0.01)\n",
    "print(sgdc)\n",
    " \n",
    "sgdc.fit(X, y)\n",
    "\n",
    "\n",
    "y_pred = sgdc.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm) \n",
    "\n",
    "result = classification_report(y_test, y_pred, digits=4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has processed  0  words\n",
      "Has processed  50  words\n",
      "Has processed  100  words\n",
      "Has processed  150  words\n",
      "Has processed  200  words\n",
      "Has processed  250  words\n",
      "Has processed  300  words\n",
      "Has processed  350  words\n",
      "Has processed  400  words\n",
      "(433, 310)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.zeros((len(mini_synsets)+len(test_synsets),310))\n",
    "all_synsets = mini_synsets + test_synsets\n",
    "\n",
    "for i, m in enumerate(all_synsets):\n",
    "    features = extracted(m,dataset.shape[1],bart_result,fre_dict,pg_score,syn_depth,syn_hypo,Ac_rating,early_learnt,bartcue_word,aoa,model)\n",
    "#     print(features)\n",
    "    dataset[i,:] = features\n",
    "    if i%50 == 0:\n",
    "        print('Has processed ',i,' words')\n",
    "        \n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_best_model_output = 999\n",
    "w2v = True\n",
    "log = True\n",
    "processing_missing_data = False\n",
    "\n",
    "if w2v:\n",
    "    dataset2 = np.zeros(dataset.shape)\n",
    "    dataset2 = dataset\n",
    "else:\n",
    "    dataset2 = np.zeros((dataset.shape[0],10))\n",
    "    dataset2 = dataset[:,:10]#number changes\n",
    "\n",
    "# process missing data, replace missing data with median of all data\n",
    "# no accuracy improvement\n",
    "# if processing_missing_data:\n",
    "#     for i in range(dataset2.shape[1]):\n",
    "#         median = np.median(dataset2[:,i][dataset2[:,i]!=0])\n",
    "#         dataset2[:,i][dataset2[:,i]==0] = median\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = np.zeros(dataset2.shape)\n",
    "for i in range(dataset2.shape[1]):\n",
    "    # take log of frequency and pagerank data\n",
    "    if log:\n",
    "        if i in [1,2,3,4,6,7,8]:\n",
    "            dataset3[:,i] = -np.log(dataset2[:,i])\n",
    "            dataset3[:,i] = (dataset3[:,i]-np.min(dataset3[:,i]))/(np.max(dataset3[:,i])-np.min(dataset3[:,i]+1e-10))\n",
    "            continue\n",
    "    dataset3[:,i] = (dataset2[:,i]-np.min(dataset2[:,i]))/(np.max(dataset2[:,i])-np.min(dataset2[:,i]+1e-10))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "prob = np.random.randint(5, size=4) /4\n",
    "print(prob)\n",
    "\n",
    "result = np.where(prob > 0.0000000001, prob, -10)\n",
    "# print(result)\n",
    "np.log10(result, out=result, where=result > 0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:len(mini_synsets)]\n",
    "y = np.array([3]*len(sub_train)+[2]*len(basic_train)+[1]*len(sup_train) + [0]*len(abst_train))\n",
    "\n",
    "# 3-way classification\n",
    "# y = np.array([3]*len(sub_train)+[2]*len(basic_train)+[1]*len(sup_train))\n",
    "\n",
    "# 4-way classification\n",
    "y_test = np.array([3]*len(sub_test) + [2]*len(basic_test) + [1]*len(sup_test) + [0]*len(abst_test))#number changes\n",
    "\n",
    "# 3-way classification\n",
    "# y_test = np.array([3]*len(sub_test) + [2]*len(basic_test) + [1]*len(sup_test))\n",
    "X_test = dataset[len(mini_synsets):]\n",
    "\n",
    "### check which feature is the most important\n",
    "### by setting other data to 0\n",
    "#0:num of features,1: frequency in corpus,2:pagerank\n",
    "#3:depth in WN,4: from bottom,5:abstractness rating,6:early learn 7:cue validity 8: number of characters 9:word2vec\n",
    "\n",
    "for data in [X, X_test]:\n",
    "#     data[:,0] = 0\n",
    "#     data[:,1] = 0\n",
    "#     data[:,2] = 0\n",
    "#     data[:,3] = 0\n",
    "#     data[:,4] = 0\n",
    "#     data[:,5] = 0\n",
    "#     data[:,6] = 0\n",
    "#     data[:,7] = 0\n",
    "#     data[:,8] = 0\n",
    "#     data[:,9] = 0\n",
    "#     data[:,10:] = 0\n",
    "    continue\n",
    "\n",
    "# revised codes\n",
    "# clf = OneVsRestClassifier(SVC()).fit(X, y)\n",
    "clf = OneVsRestClassifier(SVC(kernel = 'linear')).fit(X, y)\n",
    "y_pred = clf.predict(X_test)\n",
    "result = classification_report(y_test, y_pred, digits=4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(max_iter=15000, tol=0.01)\n",
      "[[18  1  0]\n",
      " [ 5 22  1]\n",
      " [ 5  3 32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6429    0.9474    0.7660        19\n",
      "           2     0.8462    0.7857    0.8148        28\n",
      "           3     0.9697    0.8000    0.8767        40\n",
      "\n",
      "    accuracy                         0.8276        87\n",
      "   macro avg     0.8196    0.8444    0.8192        87\n",
      "weighted avg     0.8586    0.8276    0.8326        87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.linear_model import SGDClassifier\n",
    "# clf = SGDClassifier(SVC(kernel = 'linear')).fit(X, y)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# result = classification_report(y_test, y_pred, digits=4)\n",
    "# print(result)\n",
    "\n",
    "# sgdc = SGDClassifier(max_iter=12000, tol=0.02) 87 70 74.3\n",
    "sgdc = SGDClassifier(max_iter=15000, tol=0.01)\n",
    "print(sgdc)\n",
    " \n",
    "sgdc.fit(X, y)\n",
    "\n",
    "\n",
    "y_pred = sgdc.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm) \n",
    "\n",
    "result = classification_report(y_test, y_pred, digits=4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import permutation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.999000099990002e-05"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutation_test(y_pred,y_pred2,method='approximate',num_rounds=10000,seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9665033496650335"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original codes\n",
    "clf = OneVsRestClassifier(SVC).fit(X, y)\n",
    "y_pred = clf.predict(X_test)\n",
    "result = classification_report(y_test, y_pred, digits=4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(X[X!=0]) == 0:\n",
    "    X = np.zeros((X.shape[0],1))\n",
    "else:\n",
    "    X = X[:,~np.all(X==0, axis=0)]\n",
    "if len(X_test[X_test!=0]) == 0:\n",
    "    X_test = np.zeros((X_test.shape[0],1))\n",
    "else:\n",
    "    X_test = X_test[:,~np.all(X_test==0, axis=0)]\n",
    "    \n",
    "epoch = 1\n",
    "output_size = y_test.shape[0]\n",
    "BLC_f1s = np.zeros((epoch,))\n",
    "p_values = np.zeros((epoch,))\n",
    "print(X.shape, X_test.shape)\n",
    "for i in range(save_best_model_output):\n",
    "    for i in range(epoch):\n",
    "#         clf = OneVsRestClassifier(SVC(class_weight = 'balanced',random_state = 0)).fit(X, y)\n",
    "        clf = OneVsRestClassifier(SVC()).fit(X, y)\n",
    "        sgd_predictions = clf.predict(X_test)\n",
    "#         sgd_classifier = SGDClassifier(class_weight = 'balanced',max_iter = 10000,n_iter_no_change=200)\n",
    "#         sgd_classifier.fit(X, y)\n",
    "#         sgd_predictions = sgd_classifier.predict(X_test)\n",
    "        result = classification_report(y_test, sgd_predictions, digits=4,output_dict=True)\n",
    "        BLC_f1s[i] = result['2']['f1-score']\n",
    "\n",
    "        if save_best_model_output > 1:\n",
    "            best_perm = sgd_predictions\n",
    "#             best_perm[best_perm!=2] = 0\n",
    "        elif best_perm is None:\n",
    "            pass\n",
    "        else:\n",
    "            pred_perm = sgd_predictions\n",
    "#             pred_perm[pred_perm!=2] = 0\n",
    "            p_values[i] = permutation_test(pred_perm,best_perm,method='approximate', num_rounds=10000)\n",
    "\n",
    "    # print('f1-score for BLC is ', BLC_f1s)\n",
    "    print('f1-score average -> {:.2f} ± {:.2f} %'.format(np.mean(BLC_f1s)*100,scipy.stats.sem(BLC_f1s)*100))\n",
    "    if best_perm is None:\n",
    "        print(\"Pick best model first!\")\n",
    "    elif save_best_model_output == 1:\n",
    "        print('p value of current model vs. best model -> {:.4f} ± {:.4f} '.format(np.mean(p_values),scipy.stats.sem(p_values)))\n",
    "        print('individual p value -> ', p_values)\n",
    "    if np.mean(BLC_f1s)>0.20 and save_best_model_output != 1:\n",
    "        save_best_model_output = 1\n",
    "        print('model saved!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd_predictions.tolist()\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All BLC prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if w2v:\n",
    "    dataset_all2 = dataset_all\n",
    "else:\n",
    "    dataset_all2 = dataset_all[:,:9]\n",
    "\n",
    "# process missing data, replace missing data with median of all data\n",
    "# no accuracy improvement\n",
    "if processing_missing_data:\n",
    "    for i in range(dataset_all2.shape[1]):\n",
    "        median = np.median(dataset_all2[:,i][dataset_all2[:,i]!=0])\n",
    "        dataset_all2[:,i][dataset_all2[:,i]==0] = median\n",
    "    \n",
    "dataset_all3 = np.zeros(dataset_all2.shape)\n",
    "for i in range(dataset_all2.shape[1]):\n",
    "    # take log of frequency and pagerank data\n",
    "    if log:\n",
    "        if i in [1,2,7]:\n",
    "            dataset_all3[:,i] = -np.log(dataset_all2[:,i])\n",
    "            dataset_all3[:,i] = (dataset_all3[:,i]-np.min(dataset_all3[:,i]))/(np.max(dataset_all3[:,i])-np.min(dataset_all3[:,i]))\n",
    "            continue\n",
    "    dataset_all3[:,i] = (dataset_all2[:,i]-np.min(dataset_all2[:,i]))/(np.max(dataset_all2[:,i])-np.min(dataset_all2[:,i]))\n",
    "\n",
    "\n",
    "# result = dict()\n",
    "# for i in range(dataset_all3.shape[0]):\n",
    "#     result[all_synsets2[i]] = int(clf.predict(dataset_all3[i:i+1,:7])[0])\n",
    "# # dataset_all[0].shape\n",
    "result = sgd_classifier.predict(dataset_all3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predicted_sub_02-1_sgd.txt','w') as fout:\n",
    "    for i in range(len(result)):\n",
    "        if result[i] == 3:\n",
    "            fout.write(all_synsets2[i])\n",
    "            fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predicted_BLC_02-1_sgd.txt','w') as fout:\n",
    "    for i in range(len(result)):\n",
    "        if result[i] == 2:\n",
    "            fout.write(all_synsets2[i])\n",
    "            fout.write('\\n')\n",
    "# dataset_all[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_blc = []\n",
    "with open('predicted_BLC_0131_sgd.txt','r') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        predicted_blc.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(predicted_blc)\n",
    "predicted_blc[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in predicted_blc[:250]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_word = set()\n",
    "for p in predicted_blc:\n",
    "    p = wn.synset(p).lemmas()[0]\n",
    "    predict_word.add(p.name())\n",
    "    \n",
    "print(len(predict_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predict_word_sgd798.txt','w') as fout:\n",
    "    for p in predict_word:\n",
    "        fout.write(p)\n",
    "        fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_250 = set()\n",
    "with open('predict_word_sgd798.txt','r') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if len(random_250) == 250:\n",
    "            break\n",
    "        else:\n",
    "            random_250.add(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cue validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_result['animal.n.01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_pair():\n",
    "    bart_fea = dict()\n",
    "#     with open('synset_WSD_0120_82115.csv','r') as fin:\n",
    "#     with open('synset_WSD_first_sense_0129_82115.csv','r') as fin:\n",
    "    with open('/Users/yiwen/Documents/coding_practise/blc/full_type_DM/DM_BART_82115.csv','r') as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            line = line.split(';')\n",
    "            word,features = line[0],line[1:]\n",
    "            if word in features:\n",
    "                features.remove(word)\n",
    "            bart_fea[word] = set(features)\n",
    "    return bart_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_pairs = bart_pair()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_pairs['birch.n.01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_features(dct):\n",
    "    total = []\n",
    "    for k,v in dct.items():\n",
    "        for vi in v:\n",
    "            total.append(vi)\n",
    "    return total\n",
    "\n",
    "# all_features = []\n",
    "# for k,v in features_buch.items():\n",
    "#     for vi in v:\n",
    "#         all_features.append(vi)\n",
    "\n",
    "bart_features = total_features(bart_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cue_number(total_fea):\n",
    "    q_num = dict()\n",
    "    for i in total_fea:\n",
    "        if i in q_num.keys():\n",
    "            q_num[i]+=1\n",
    "        else:\n",
    "            q_num[i]=0\n",
    "            q_num[i]+=1\n",
    "    return q_num\n",
    "\n",
    "bartcue_num = cue_number(bart_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bartcue_num['cat.n.01']#有33个synsets带有cat.n.01这个feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cue_category(q_num,total_fea,all_words):\n",
    "    cue_ctgy = dict()\n",
    "    for i in all_words:\n",
    "        c = 0\n",
    "        for f in total_fea[i]:\n",
    "            mks = 1/q_num[f]\n",
    "            c+=mks\n",
    "        cue_ctgy[i] = c\n",
    "    return cue_ctgy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = lambda s: s.hypernyms()\n",
    "list(wn.synset('sea.n.01').closure(hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('thing.n.12').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_(syn):\n",
    "    hp = lambda s: s.hypernyms()\n",
    "    tmp =  list(wn.synset(syn).closure(hp))\n",
    "    up = []\n",
    "    for t in tmp:\n",
    "        up.append(t.name())\n",
    "    return up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['animal.n.01','plant.n.02','artifact.n.01','natural_phenomenon.n.01','matter.n.03','thing.n.12']\n",
    "bart_world = set()\n",
    "for k in bart_pairs.keys():\n",
    "    up = hyper_(k)\n",
    "    for c in categories:\n",
    "        if c in up:\n",
    "            bart_world.add(k)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bartcue_word = cue_category(bartcue_num,bart_pairs,bart_pairs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bartcue_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('cue_validity_82115_1st.csv','w') as fout:\n",
    "with open('cue_validity_dm_bart_82115.csv','w') as fout:\n",
    "    for k,v in bartcue_word.items():\n",
    "        content = k + '\\t' + str(v) + '\\n'\n",
    "        fout.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_cue_word_sorted = sorted(bartcue_word.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data = []\n",
    "with open('/Users/yiwen/Documents/coding_practise/blc/Final_dataset/synset_word_40_mapping.txt','r') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        line = line.split('\\t')\n",
    "        syn = line[0]\n",
    "        mini_data.append(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = mini_data [0:10]\n",
    "basic = mini_data [11:20]\n",
    "sup = mini_data[21:30]\n",
    "abst = mini_data[31:40]\n",
    "\n",
    "categories = [sub,basic,sup,abst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in categories:\n",
    "    avg = 0.0\n",
    "    for w in c:\n",
    "        avg += float(bartcue_word[w])\n",
    "    print(c,avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_list = ['bird','penguin','eagle','emu','falcon','owl','robin','parakeet','partridge','peacock','pelican','gull','seagull','pheasant','pigeon','quill']\n",
    "fish_list = ['fish','reef','shark','dolphin','whale','rod','salamander','salmon','sardine','walrus','mackerel','pike','tuna','trout']\n",
    "domestic_list = ['rooster','duck','swan','chicken','pig','cow','ox','sheep','ram','ewe','goat']\n",
    "sea_animal = ['seafood','turtle','lobster','oyster','octopus','shell','clam','snail','platypus','squid','shrimp']\n",
    "food_list = ['meat','flesh','snack','cornbeef','beef','fry','veal','mutton','lamb','toast','hamburger','gravy','bouillon','bread','pizza','pepperoni','bologna','pie','pastry','pasta','cake','pancake','noodle']\n",
    "spice_list = ['spice','garlic','sugar','herb','pepper','salt','parsley','herb','mint','oregano','onion']\n",
    "horse_list = ['horse','bronco','zebra','donkey','colt']\n",
    "insect_list = ['insect','hornet','bee','butterfly']\n",
    "rodent_list = ['rodent','mouse','chipmunk','hamster','porcupine','squirrel']\n",
    "mammal_list = ['mammal','cat','tiger','lion','leopard','cheetah','cougar','bat_animal','seal','beaver','otter','mink','mongoose','bison','buffalo','bull','camel','deer','caribou','moose','elk','fawn','dog','fox','wolf','coyote','hyena','chimp','monkey','gorilla','chimpanzee','elephant','giraffe','rabbit']\n",
    "animal_list = ['animal','snake','reptile','lizard','python','rattlesnake','bear','grizzly']\n",
    "flower_list = ['flower','violet','dandelion','rose','tulip','orchid']\n",
    "vegetable_list = ['vegetable','zucchini','cucumber','cabbage','carrot','yam','turnip','tomato','spinach','rhubarb','radish','pumpkin','potato','pickle','pea','cauliflower','artichoke','bean']\n",
    "plant_list = ['plant','tree','seed','oak','birch','cactus','forest','wood','willow','vine','shrub','seaweed','coral','wheat','oat','weed','grass','nut','peanut','walnut','mushroom','fungus','moss']\n",
    "fruit_list = ['fruit','grapefruit','apple','orange','tangerine','walnut','watermelon','cantaloupe','cherry','strawberry','raspberry','blueberry','raisin','plum','pineapple','pear','peach','olive','mandarin','nectarine','lime','lemon','grape','fig','coconut','banana']\n",
    "vehicle_list = ['vehicle','submarine','ship','boat','train','bus','subway','helicopter','jeep','van','car','cab','caravan','scooter','jet','plane','truck','bicycle','tricycle','unicycle','motorcycle']\n",
    "all_list = bird_list + fish_list + insect_list + sea_animal + domestic_list + food_list + spice_list + horse_list + animal_list + mammal_list + rodent_list + plant_list + flower_list +vehicle_list + vegetable_list +fruit_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_names = [bird_list,fish_list,domestic_list,food_list,spice_list,sea_animal,horse_list,animal_list,insect_list,mammal_list,rodent_list,flower_list,vegetable_list,plant_list,fruit_list,vehicle_list,]\n",
    "len_animals = len(bird_list) + len(fish_list) + len(domestic_list) + len(sea_animal) + len(horse_list) + len(insect_list) + len(rodent_list) + len(mammal_list) + len(animal_list)\n",
    "print(len_animals,'animals')\n",
    "len_plants = len(spice_list) + len(flower_list) + len(vegetable_list) + len(plant_list) + len(fruit_list)\n",
    "print(len_plants,'plants')\n",
    "print(len(vehicle_list),'vehicles')\n",
    "print(len(food_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "features_buch = dict()\n",
    "with open('/Users/yiwen/Documents/coding_practise/blc/Buch_4436_processed.csv','r') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        line = line.split(';')\n",
    "        word,feature = line[0],line[1]\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        if word == 'bike':\n",
    "            word = 'bicycle'\n",
    "            features_buch[word].add(feature)\n",
    "        else:\n",
    "            if word in all_list:\n",
    "                if word in features_buch.keys():\n",
    "                    features_buch[word].add(feature)\n",
    "                else:\n",
    "                    features_buch[word] = set()\n",
    "                    features_buch[word].add(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_features(dct):\n",
    "    total = []\n",
    "    for k,v in dct.items():\n",
    "        for vi in v:\n",
    "            total.append(vi)\n",
    "    return total\n",
    "\n",
    "# all_features = []\n",
    "# for k,v in features_buch.items():\n",
    "#     for vi in v:\n",
    "#         all_features.append(vi)\n",
    "\n",
    "all_features = total_features(features_buch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cue_number(total_fea):\n",
    "    q_num = dict()\n",
    "    for i in total_fea:\n",
    "        if i in q_num.keys():\n",
    "            q_num[i]+=1\n",
    "        else:\n",
    "            q_num[i]=0\n",
    "            q_num[i]+=1\n",
    "    return q_num\n",
    "\n",
    "cue_num = cue_number(all_features)\n",
    "# cue_num = dict()\n",
    "# for a in all_features:\n",
    "#     if a in cue_num.keys():\n",
    "#         cue_num[a]+=1\n",
    "#     else:\n",
    "#         cue_num[a] = 0\n",
    "#         cue_num[a]+=1\n",
    "\n",
    "def cue_category(q_num,total_fea,all_words):\n",
    "    cue_ctgy = dict()\n",
    "    for i in all_words:\n",
    "        c = 0\n",
    "        for f in total_fea[i]:\n",
    "            mks = 1/q_num[f]\n",
    "            c+=mks\n",
    "        cue_ctgy[i] = c\n",
    "    return cue_ctgy\n",
    "\n",
    "# cue_word = dict()\n",
    "# for i in all_list:\n",
    "#     init = 0\n",
    "#     for f in features_buch[i]:\n",
    "#         mks = 1/cue_num[f]\n",
    "#         init += mks\n",
    "#     cue_word[i] = init\n",
    "\n",
    "cue_word = cue_category(cue_num,features_buch,all_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_score = dict()\n",
    "for n in list_names:\n",
    "    category_score[n[0]] = dict()\n",
    "    for i in n:\n",
    "        print(i,cue_word[i])\n",
    "        category_score[n[0]][i] = cue_word[i]\n",
    "    print(len(n),' ',n[0],'____')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_list = ['vehicle','transportation','animal','plant','fruit','bird','feather']\n",
    "features_buch2 = dict()\n",
    "with open('/Users/yiwen/Documents/coding_practise/blc/Buch_4436_processed.csv','r') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        line = line.split(';')\n",
    "        word,feature = line[0],line[1]\n",
    "        if line[2] == 'noun':\n",
    "            if word in features_buch2.keys():\n",
    "                features_buch2[word].add(feature)\n",
    "            else:\n",
    "                features_buch2[word] = set()\n",
    "                features_buch2[word].add(feature)\n",
    "\n",
    "selected_word = set()\n",
    "\n",
    "for k,v in features_buch2.items():\n",
    "    for vi in v:\n",
    "        if vi in type_list:\n",
    "            selected_word.add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features2 = total_features(features_buch2)\n",
    "cue_num2 = cue_number(all_features2)\n",
    "cue_word2 = cue_category(cue_num2,features_buch2,selected_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
